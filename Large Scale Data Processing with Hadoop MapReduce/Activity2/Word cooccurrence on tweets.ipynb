{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                                                         Saurabh Bajoria\n",
    "                                                                                                         UBid: sbajoria\n",
    "                                                                                                         Person no:50208005\n",
    "\n",
    "**CSE 4/587 Data Intensive Computing**\n",
    "====================\n",
    "***LAB3: LARGE SCALE DATA (TEXT) PROCESSING WITH HADOOP MAPREDUCE***\n",
    "---------------------------------------\n",
    "                                                                                                       \n",
    "\n",
    "*Word co-occurrence on tweets*\n",
    "--------------------------------------\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "- Installing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/Saurabh/Documents/R/win-library/3.3'\n",
      "(as 'lib' is unspecified)\n",
      "Installing package into 'C:/Users/Saurabh/Documents/R/win-library/3.3'\n",
      "(as 'lib' is unspecified)\n",
      "Installing package into 'C:/Users/Saurabh/Documents/R/win-library/3.3'\n",
      "(as 'lib' is unspecified)\n",
      "Installing package into 'C:/Users/Saurabh/Documents/R/win-library/3.3'\n",
      "(as 'lib' is unspecified)\n",
      "Installing package into 'C:/Users/Saurabh/Documents/R/win-library/3.3'\n",
      "(as 'lib' is unspecified)\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"twitteR\",repos=\"cran.r-project.org\")\n",
    "install.packages(\"wordcloud\",repos=\"cran.r-project.org\")\n",
    "install.packages(\"RColorBrewer\",repos=\"cran.r-project.org\")\n",
    "install.packages(\"stringr\",repos=\"cran.r-project.org\")\n",
    "install.packages(\"wordcloud\",repos=\"cran.r-project.org\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Including the libraries installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(lubridate)\n",
    "library(twitteR)\n",
    "library(wordcloud)\n",
    "library(RColorBrewer)\n",
    "library(stringr)\n",
    "library(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Authentication for Twitter API, required to extract tweets from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Using direct authentication\"\n"
     ]
    }
   ],
   "source": [
    "consumer_key <-'0rnEKoYQGRaumOd70HSbwpSM6'\n",
    "consumer_secret <- 'kmqdpKcMHsOqeVY2MgSTXMPoZsLTLlyrxlIEmvJPb7ULp6Gjh6'\n",
    "access_token <- '805074888419635202-i0IlVwS7UR7vq7sMg9BSY431VyJ2Tyj'\n",
    "access_secret <- 'P089q9h1GHxw9ER9UCB5ZqM4Y1ng7uH5VmaT8aubK6drt'\n",
    "setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the following three key words to extract tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1=\"soccer\"\n",
    "text2=\"economy\"\n",
    "text3=\"cricket\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extracting the tweets for all the keywords across last 10 days\n",
    "- Using the lubridate package to manipulate date and retrieve tweets\n",
    "- Also, stripping the retweets to avoid duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"2017-04-13\"\n",
      "[1] \"2017-04-14\"\n",
      "[1] \"2017-04-14\"\n",
      "[1] \"2017-04-15\"\n",
      "[1] \"2017-04-15\"\n",
      "[1] \"2017-04-16\"\n",
      "[1] \"2017-04-16\"\n",
      "[1] \"2017-04-17\"\n",
      "[1] \"2017-04-17\"\n",
      "[1] \"2017-04-18\"\n",
      "[1] \"2017-04-18\"\n",
      "[1] \"2017-04-19\"\n",
      "[1] \"2017-04-19\"\n",
      "[1] \"2017-04-20\"\n",
      "[1] \"2017-04-20\"\n",
      "[1] \"2017-04-21\"\n",
      "[1] \"2017-04-21\"\n",
      "[1] \"2017-04-22\"\n",
      "[1] \"2017-04-22\"\n",
      "[1] \"2017-04-23\"\n"
     ]
    }
   ],
   "source": [
    "untilDate=Sys.Date()-10\n",
    "for(i in 1:10) {\n",
    "    sinceDate=untilDate\n",
    "    untilDate=ymd(sinceDate) + days(1)\n",
    "    print(sinceDate)\n",
    "    print(untilDate)\n",
    "tweetList1 <- searchTwitter(text1,n=100,since=toString(sinceDate), until=toString(untilDate))\n",
    "    tweetList1<-strip_retweets(tweetList1)\n",
    "tweetList2 <- searchTwitter(text2,n=100,since=toString(sinceDate), until=toString(untilDate))\n",
    "    tweetList2<-strip_retweets(tweetList2)\n",
    "tweetList3 <- searchTwitter(text3,n=100,since=toString(sinceDate), until=toString(untilDate))\n",
    "    tweetList3<-strip_retweets(tweetList3)\n",
    "tweetsDf1<- twListToDF(tweetList1)\n",
    "tweetsDf2<- twListToDF(tweetList2)\n",
    "tweetsDf3<- twListToDF(tweetList3)\n",
    "write(tweetsDf1$text, file = paste(\"Tweets\\\\RawTweets\\\\soccer\",i,\".txt\", sep=\"\"),append=FALSE)\n",
    "write(tweetsDf2$text, file = paste(\"Tweets\\\\RawTweets\\\\economy\",i,\".txt\", sep=\"\"),append=FALSE)\n",
    "write(tweetsDf3$text, file = paste(\"Tweets\\\\RawTweets\\\\cricket\",i,\".txt\", sep=\"\"),append=FALSE)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the next steps, please read the **ReadmeActivity2.txt** to follow the steps for preprocessing the tweets and running WordCooccurrence on it using Hadoop Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
